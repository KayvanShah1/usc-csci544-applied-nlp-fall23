{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-autotime in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (0.3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: ipython in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython-autotime) (8.15.0)\n",
      "Requirement already satisfied: backcall in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (5.10.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (1.1.3)\n",
      "Requirement already satisfied: colorama in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-autotime) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython->ipython-autotime) (1.16.0)\n",
      "Requirement already satisfied: contractions in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "time: 3.38 s (started: 2023-09-24 17:59:45 -07:00)\n"
     ]
    }
   ],
   "source": [
    "%pip install ipython-autotime\n",
    "%pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 13:29:17 -07:00)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2023-09-26 22:56:21 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class PathConfig:\n",
    "    HW2_DIR = os.path.dirname(os.getcwd())\n",
    "    OUTPUT_DIR = os.path.join(HW2_DIR, \"solution\", \"output\")\n",
    "\n",
    "    DATA_PATH = os.path.join(HW2_DIR, \"CSCI544_HW2\", \"data\")\n",
    "    VERIFICATION_DATA_PATH = os.path.join(HW2_DIR, \"CSCI544_HW2\", \"verification\")\n",
    "\n",
    "    VOCAB_FILE_PATH = os.path.join(OUTPUT_DIR, \"vocab.txt\")\n",
    "    HMM_MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, \"hmm.json\")\n",
    "    GREEDY_ALGO_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"greedy.json\")\n",
    "    VITERBI_ALGO_OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"viterbi.json\")\n",
    "\n",
    "\n",
    "class WSJDatasetConfig:\n",
    "    cols = [\"index\", \"sentences\", \"labels\"]\n",
    "\n",
    "    train_file_path = os.path.join(PathConfig.DATA_PATH, \"train.json\")\n",
    "    dev_file_path = os.path.join(PathConfig.DATA_PATH, \"dev.json\")\n",
    "    test_file_path = os.path.join(PathConfig.DATA_PATH, \"test.json\")\n",
    "\n",
    "\n",
    "class VocabConfig:\n",
    "    UNKNOWN_TOKEN = \"<unk>\"\n",
    "    THRESHOLD = 2\n",
    "    FILE_HEADER = [\"word\", \"index\", \"frequency\"]\n",
    "\n",
    "    VOCAB_FILE = PathConfig.VOCAB_FILE_PATH\n",
    "\n",
    "\n",
    "class HMMConfig:\n",
    "    HMM_MODEL_SAVED = PathConfig.HMM_MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-09-26 22:49:44 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class WSJDataset:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        self.data: pd.DataFrame = None\n",
    "        self.cols = WSJDatasetConfig.cols\n",
    "\n",
    "    def read_data(self):\n",
    "        self.data = pd.read_json(self.path)\n",
    "\n",
    "    def process_sentences(self):\n",
    "        self.data[\"sentence\"] = self.data[\"sentence\"].apply(\n",
    "            lambda sentence: [word.lower() for word in sentence],\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        self.read_data()\n",
    "        self.process_sentences()\n",
    "        return self.data\n",
    "\n",
    "    def get_sentences_with_pos_tags(self):\n",
    "        if \"labels\" in self.data.columns:\n",
    "            sentences_with_pos_tags = self.data.loc[:, [\"sentence\", \"labels\"]].apply(\n",
    "                lambda row: list(zip(row[\"sentence\"], row[\"labels\"])), axis=1\n",
    "            )\n",
    "        else:\n",
    "            sentences_with_pos_tags = self.data[\"sentence\"].apply(\n",
    "                lambda sentence: list(zip(sentence, [None] * len(sentence)))\n",
    "            )\n",
    "        sentences_with_pos_tags = sentences_with_pos_tags.tolist()\n",
    "        return sentences_with_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38218, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[mr., vinken, is, chairman, of, elsevier, n.v....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[rudolph, agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[a, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[the, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  \\\n",
       "0      0  [pierre, vinken, ,, 61, years, old, ,, will, j...   \n",
       "1      1  [mr., vinken, is, chairman, of, elsevier, n.v....   \n",
       "2      2  [rudolph, agnew, ,, 55, years, old, and, forme...   \n",
       "3      3  [a, form, of, asbestos, once, used, to, make, ...   \n",
       "4      4  [the, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  \n",
       "1  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  \n",
       "2  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  \n",
       "3  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "4  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  "
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.17 s (started: 2023-09-26 22:49:47 -07:00)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = WSJDataset(path=WSJDatasetConfig.train_file_path)\n",
    "df_train = train_dataset.prepare_dataset()\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5527, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, arizona, corporations, commission, autho...</td>\n",
       "      <td>[DT, NNP, NNP, NNP, VBD, DT, CD, NN, NN, NN, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, ruling, follows, a, host, of, problems, ...</td>\n",
       "      <td>[DT, NN, VBZ, DT, NN, IN, NNS, IN, NNP, NNP, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[the, arizona, regulatory, ruling, calls, for,...</td>\n",
       "      <td>[DT, NNP, JJ, NN, VBZ, IN, $, CD, CD, IN, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[the, company, had, sought, increases, totalin...</td>\n",
       "      <td>[DT, NN, VBD, VBN, NNS, VBG, $, CD, CD, ,, CC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[the, decision, was, announced, after, trading...</td>\n",
       "      <td>[DT, NN, VBD, VBN, IN, NN, VBD, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  \\\n",
       "0      0  [the, arizona, corporations, commission, autho...   \n",
       "1      1  [the, ruling, follows, a, host, of, problems, ...   \n",
       "2      2  [the, arizona, regulatory, ruling, calls, for,...   \n",
       "3      3  [the, company, had, sought, increases, totalin...   \n",
       "4      4  [the, decision, was, announced, after, trading...   \n",
       "\n",
       "                                              labels  \n",
       "0  [DT, NNP, NNP, NNP, VBD, DT, CD, NN, NN, NN, I...  \n",
       "1  [DT, NN, VBZ, DT, NN, IN, NNS, IN, NNP, NNP, ,...  \n",
       "2  [DT, NNP, JJ, NN, VBZ, IN, $, CD, CD, IN, JJ, ...  \n",
       "3  [DT, NN, VBD, VBN, NNS, VBG, $, CD, CD, ,, CC,...  \n",
       "4                 [DT, NN, VBD, VBN, IN, NN, VBD, .]  "
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms (started: 2023-09-26 22:49:51 -07:00)\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = WSJDataset(path=WSJDatasetConfig.dev_file_path)\n",
    "df_valid = valid_dataset.prepare_dataset()\n",
    "print(df_valid.shape)\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5462, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[influential, members, of, the, house, ways, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, bill, ,, whose, backers, include, chairm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[the, bill, intends, to, restrict, the, rtc, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[``, such, agency, `, self-help, ', borrowing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[the, complex, financing, plan, in, the, s&amp;l, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence\n",
       "0      0  [influential, members, of, the, house, ways, a...\n",
       "1      1  [the, bill, ,, whose, backers, include, chairm...\n",
       "2      2  [the, bill, intends, to, restrict, the, rtc, t...\n",
       "3      3  [``, such, agency, `, self-help, ', borrowing,...\n",
       "4      4  [the, complex, financing, plan, in, the, s&l, ..."
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms (started: 2023-09-26 22:49:53 -07:00)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = WSJDataset(path=WSJDatasetConfig.test_file_path)\n",
    "df_test = test_dataset.prepare_dataset()\n",
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Vocabulary Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 22:50:09 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class VocabularyGenerator:\n",
    "    def __init__(\n",
    "        self, threshold: int, unknown_token: str = None, save: bool = False, path: str = None\n",
    "    ):\n",
    "        \"\"\"Initialize a VocabularyGenerator\n",
    "\n",
    "        Args:\n",
    "            threshold (int): Frequency threshold for rare words.\n",
    "            unknown_token (str, optional): Token to replace rare words. Defaults to None.\n",
    "            save (bool, optional): Flag to save the vocabulary. Default is True.\n",
    "            path (str, optional): Path to save the vocabulary. Defaults to None.\n",
    "\n",
    "        Usage:\n",
    "            vocab_generator = VocabularyGenerator(threshold=3, unknown_token=\"<unk>\")\n",
    "            vocab_df = vocab_generator.generate_vocabulary(data, \"sentence\")\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.unknown_token = (\n",
    "            unknown_token if unknown_token is not None else VocabConfig.UNKNOWN_TOKEN\n",
    "        )\n",
    "        self._save = save\n",
    "\n",
    "        if self._save and path is None:\n",
    "            self.path = VocabConfig.VOCAB_FILE\n",
    "        else:\n",
    "            self.path = path\n",
    "\n",
    "    def _count_word_frequency(self, data, sentence_col_name):\n",
    "        word_freq = (\n",
    "            data[sentence_col_name]\n",
    "            .explode()\n",
    "            .value_counts()\n",
    "            .rename_axis(\"word\")\n",
    "            .reset_index(name=\"frequency\")\n",
    "        )\n",
    "        return word_freq\n",
    "\n",
    "    def generate_vocabulary(self, data: pd.DataFrame, sentence_col_name: str):\n",
    "        \"\"\"Generate a vocabulary from the provided dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The DataFrame containing the dataset.\n",
    "            sentence_col_name (str): The name of the column containing sentences.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with the generated vocabulary.\n",
    "\n",
    "        This method takes a DataFrame with sentences and generates a vocabulary based on word frequencies.\n",
    "        It replaces words with frequencies less than the specified threshold with the unknown token (\"<unk>\").\n",
    "        The resulting DataFrame is sorted by frequency and indexed.\n",
    "\n",
    "        If the 'save' flag is set, the vocabulary will be saved to the specified path.\n",
    "\n",
    "        Usage:\n",
    "            ```py\n",
    "            vocab_generator = VocabularyGenerator(threshold=3, unknown_token=\"<unk>\")\n",
    "            vocab_df = vocab_generator.generate_vocabulary(data, sentence_col_name)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        word_freq_df = self._count_word_frequency(data, sentence_col_name)\n",
    "\n",
    "        # Replace words with frequency less than threshold with '<unk>'\n",
    "        word_freq_df[\"word\"] = word_freq_df.apply(\n",
    "            lambda row: self.unknown_token if row[\"frequency\"] <= self.threshold else row[\"word\"],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # Group by 'Word' and aggregate by sum\n",
    "        word_freq_df = word_freq_df.groupby(\"word\", as_index=False)[\"frequency\"].agg(\"sum\")\n",
    "\n",
    "        # Sort the DataFrame by frequency\n",
    "        word_freq_df = word_freq_df.sort_values(by=\"frequency\", ascending=False, ignore_index=True)\n",
    "\n",
    "        # Placing Special Tokens at the top of the DataFrame\n",
    "        unk_df = word_freq_df.loc[word_freq_df[\"word\"] == self.unknown_token]\n",
    "        word_freq_df = word_freq_df.loc[word_freq_df[\"word\"] != self.unknown_token]\n",
    "\n",
    "        word_freq_df = pd.concat([unk_df, word_freq_df], ignore_index=True)\n",
    "\n",
    "        # Add an index column\n",
    "        word_freq_df[\"index\"] = range(len(word_freq_df))\n",
    "\n",
    "        if self._save:\n",
    "            self.save_vocab(word_freq_df, self.path)\n",
    "\n",
    "        return word_freq_df\n",
    "\n",
    "    def save_vocab(self, word_freq_df, path):\n",
    "        \"\"\"Write your vocabulary to the file\"\"\"\n",
    "        if not os.path.exists(os.path.dirname(path)):\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "\n",
    "        with open(path, \"w\") as file:\n",
    "            vocabulary = word_freq_df.to_records(index=False)\n",
    "            for word, frequency, index in vocabulary:\n",
    "                file.write(f\"{word}\\t{index}\\t{frequency}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>28581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>46476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>46144</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>37452</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>22176</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>21459</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>19338</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>16320</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>15875</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'s</td>\n",
       "      <td>8886</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  frequency  index\n",
       "0  <unk>      28581      0\n",
       "1      ,      46476      1\n",
       "2    the      46144      2\n",
       "3      .      37452      3\n",
       "4     of      22176      4\n",
       "5     to      21459      5\n",
       "6      a      19338      6\n",
       "7     in      16320      7\n",
       "8    and      15875      8\n",
       "9     's       8886      9"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 s (started: 2023-09-26 22:50:10 -07:00)\n"
     ]
    }
   ],
   "source": [
    "vocab_generator = VocabularyGenerator(\n",
    "    threshold=VocabConfig.THRESHOLD, unknown_token=VocabConfig.UNKNOWN_TOKEN, save=True\n",
    ")\n",
    "vocab_df = vocab_generator.generate_vocabulary(df_train, \"sentence\")\n",
    "vocab_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected threshold for unknown words:  2\n",
      "Vocabulary size:  15568\n",
      "Total occurrences of the special token <unk>:  28581\n",
      "time: 16 ms (started: 2023-09-26 22:50:12 -07:00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected threshold for unknown words: \", VocabConfig.THRESHOLD)\n",
    "print(\"Vocabulary size: \", vocab_df.shape[0])\n",
    "print(\n",
    "    \"Total occurrences of the special token <unk>: \",\n",
    "    int(vocab_df[vocab_df[\"word\"] == \"<unk>\"].frequency),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique POS tags = 45\n",
      "Unique Part-of-speech tags:\n",
      " ['NNP' ',' 'CD' 'NNS' 'JJ' 'MD' 'VB' 'DT' 'NN' 'IN' '.' 'VBZ' 'VBG' 'CC'\n",
      " 'VBD' 'VBN' 'RB' 'TO' 'PRP' 'RBR' 'WDT' 'VBP' 'RP' 'PRP$' 'JJS' 'POS'\n",
      " '``' 'EX' \"''\" 'WP' ':' 'JJR' 'WRB' '$' 'NNPS' 'WP$' '-LRB-' '-RRB-'\n",
      " 'PDT' 'RBS' 'FW' 'UH' 'SYM' 'LS' '#']\n",
      "time: 188 ms (started: 2023-09-26 22:50:14 -07:00)\n"
     ]
    }
   ],
   "source": [
    "unique_pos_tags = df_train.labels.explode().unique()\n",
    "print(\"Number of unique POS tags =\", unique_pos_tags.shape[0])\n",
    "print(\"Unique Part-of-speech tags:\\n\", unique_pos_tags)\n",
    "unique_pos_tags = unique_pos_tags.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.62 s (started: 2023-09-26 22:50:16 -07:00)\n"
     ]
    }
   ],
   "source": [
    "train_sentences_with_pos_tags = train_dataset.get_sentences_with_pos_tags()\n",
    "valid_sentences_with_pos_tags = valid_dataset.get_sentences_with_pos_tags()\n",
    "test_sentences_with_pos_tags = test_dataset.get_sentences_with_pos_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pierre', 'NNP'),\n",
       " ('vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 22:50:18 -07:00)\n"
     ]
    }
   ],
   "source": [
    "train_sentences_with_pos_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 22:50:45 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class HMM:\n",
    "    def __init__(self, vocab_file: str, labels: List[str]):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            train_data (pd.DataFrame): _description_\n",
    "            vocab_file (str): _description_\n",
    "        \"\"\"\n",
    "        self.vocab = self._read_vocab(vocab_file)\n",
    "        self.labels = labels\n",
    "\n",
    "        # Hidden Markov Model Parameters\n",
    "        self.states = list()\n",
    "        self.priors = None\n",
    "        self.transitions = None\n",
    "        self.emissions = None\n",
    "\n",
    "        # Laplace Smoothing\n",
    "        self.smoothing_constant = 1e-10\n",
    "\n",
    "    def _read_vocab(self, vocab_file: str):\n",
    "        return pd.read_csv(vocab_file, sep=\"\\t\", names=VocabConfig.FILE_HEADER)\n",
    "\n",
    "    def _initialize_params(self):\n",
    "        self.states = list(self.labels)\n",
    "\n",
    "        # N = Number of states i.e. number of distinct tags\n",
    "        num_states = len(self.labels)\n",
    "        # M = Number of observable symbols i.e. number of distinct words\n",
    "        num_observations = len(self.vocab)\n",
    "\n",
    "        # State transition probability matrix of size N * N\n",
    "        self.transitions = np.zeros((num_states, num_states))\n",
    "\n",
    "        # Obseravtion Emission probability matrix of size N * M\n",
    "        self.emissions = np.zeros((num_states, num_observations))\n",
    "\n",
    "        # Prior probability matrix of size N * 1\n",
    "        self.priors = np.zeros(num_states)\n",
    "\n",
    "    def _smoothen_propabilities(self, prob_mat: np.array, smoothing_constant: float):\n",
    "        \"\"\"Handle cases where the probabilities is 0\"\"\"\n",
    "        return np.where(prob_mat == 0, smoothing_constant, prob_mat)\n",
    "\n",
    "    def _compute_prior_params(self, train_data):\n",
    "        tag_to_index = {tag: i for i, tag in enumerate(self.labels)}\n",
    "        num_sentences = len(train_data)\n",
    "\n",
    "        for sentence in train_data:\n",
    "            label = sentence[0][1]\n",
    "            state_idx = tag_to_index[label]\n",
    "            self.priors[state_idx] += 1\n",
    "\n",
    "        self.priors = self.priors / num_sentences\n",
    "        self.priors = self._smoothen_propabilities(self.priors, self.smoothing_constant)\n",
    "\n",
    "    def _compute_transition_params(self, train_data):\n",
    "        tag_to_index = {tag: i for i, tag in enumerate(self.labels)}\n",
    "\n",
    "        for sentence in train_data:\n",
    "            label_indices = [tag_to_index.get(label) for _, label in sentence]\n",
    "\n",
    "            for i in range(1, len(label_indices)):\n",
    "                prev_state = label_indices[i - 1]\n",
    "                curr_state = label_indices[i]\n",
    "                self.transitions[prev_state, curr_state] += 1\n",
    "\n",
    "        row_agg = self.transitions.sum(axis=1)[:, np.newaxis]\n",
    "        self.transitions = self.transitions / row_agg\n",
    "        self.transitions = self._smoothen_propabilities(self.transitions, self.smoothing_constant)\n",
    "\n",
    "    def _compute_emission_params(self, train_data):\n",
    "        word_to_index = dict(zip(self.vocab[\"word\"], self.vocab[\"index\"]))\n",
    "        tag_to_index = {tag: i for i, tag in enumerate(self.labels)}\n",
    "\n",
    "        for sentence in train_data:\n",
    "            for word, label in sentence:\n",
    "                state_idx = tag_to_index[label]\n",
    "                word_idx = word_to_index.get(word, word_to_index[VocabConfig.UNKNOWN_TOKEN])\n",
    "                self.emissions[state_idx, word_idx] += 1\n",
    "\n",
    "        row_agg = self.emissions.sum(axis=1)[:, np.newaxis]\n",
    "        self.emissions = self.emissions / row_agg\n",
    "        self.emissions = self._smoothen_propabilities(self.emissions, self.smoothing_constant)\n",
    "\n",
    "    def fit(self, train_data: pd.DataFrame):\n",
    "        self._initialize_params()\n",
    "        self._compute_prior_params(train_data)\n",
    "        self._compute_transition_params(train_data)\n",
    "        self._compute_emission_params(train_data)\n",
    "\n",
    "    @property\n",
    "    def get_all_probability_matrices(self):\n",
    "        return self.priors, self.transitions, self.emissions\n",
    "\n",
    "    def save_model(self, file_path=None):\n",
    "        if file_path is None:\n",
    "            file_path = HMMConfig.HMM_MODEL_SAVED\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "\n",
    "        transition_prob = {\n",
    "            f\"({s1}, {s2})\": self.transitions[self.states.index(s1), self.states.index(s2)]\n",
    "            for s1, s2 in itertools.product(self.states, repeat=2)\n",
    "        }\n",
    "\n",
    "        emission_prob = {\n",
    "            f\"({s}, {w})\": p\n",
    "            for s in self.states\n",
    "            for w, p in zip(self.vocab[\"word\"], self.emissions[self.states.index(s), :])\n",
    "        }\n",
    "\n",
    "        model_params = {\"transition\": transition_prob, \"emission\": emission_prob}\n",
    "\n",
    "        with open(file_path, \"w\") as json_file:\n",
    "            json.dump(model_params, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.69 s (started: 2023-09-26 22:50:46 -07:00)\n"
     ]
    }
   ],
   "source": [
    "model = HMM(vocab_file=VocabConfig.VOCAB_FILE, labels=unique_pos_tags)\n",
    "model.fit(train_sentences_with_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Transition Parameters = 2025\n",
      "Number of Emission Parameters = 700560\n",
      "time: 0 ns (started: 2023-09-26 22:50:48 -07:00)\n"
     ]
    }
   ],
   "source": [
    "p, t, e = model.get_all_probability_matrices\n",
    "print(\"Number of Transition Parameters =\", len(t.flatten()))\n",
    "print(\"Number of Emission Parameters =\", len(e.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.64 s (started: 2023-09-26 22:51:04 -07:00)\n"
     ]
    }
   ],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 22:50:53 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class GreedyDecoding:\n",
    "    def __init__(self, prior_probs, transition_probs, emission_probs, states, vocab):\n",
    "        self.priors = prior_probs\n",
    "        self.transitions = transition_probs\n",
    "        self.emissions = emission_probs\n",
    "        self.states = states\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.tag_to_idx = {tag: idx for idx, tag in enumerate(states)}\n",
    "        self.word_to_index = dict(zip(self.vocab[\"word\"], self.vocab[\"index\"]))\n",
    "\n",
    "        # Precompute scores for each word-tag pair\n",
    "        self.priors_emissions = prior_probs[:, np.newaxis] * emission_probs\n",
    "\n",
    "    def _decode_single_sentence(self, sentence):\n",
    "        predicted_tags = []\n",
    "\n",
    "        prev_tag_idx = None\n",
    "\n",
    "        for word in sentence:\n",
    "            word_idx = self.word_to_index.get(word, self.word_to_index[VocabConfig.UNKNOWN_TOKEN])\n",
    "\n",
    "            if prev_tag_idx is None:\n",
    "                # scores = self.priors * self.emissions[:, word_idx]\n",
    "                scores = self.priors_emissions[:, word_idx]\n",
    "            else:\n",
    "                scores = self.transitions[prev_tag_idx] * self.emissions[:, word_idx]\n",
    "\n",
    "            prev_tag_idx = np.argmax(scores)\n",
    "            predicted_tags.append(self.states[prev_tag_idx])\n",
    "\n",
    "        return predicted_tags\n",
    "\n",
    "    def decode(self, sentences):\n",
    "        predicted_tags_list = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            predicted_tags = self._decode_single_sentence([word for word, tag in sentence])\n",
    "            predicted_tags_list.append(predicted_tags)\n",
    "\n",
    "        return predicted_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 22:50:54 -07:00)\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predicted_sequences, true_sequences):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predicted sequences compared to true sequences.\n",
    "\n",
    "    Args:\n",
    "        predicted_sequences (list): List of predicted sequences.\n",
    "        true_sequences (list): List of true sequences.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    assert len(predicted_sequences) == len(true_sequences), \"Lists must have the same length.\"\n",
    "\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    for true_label, predicted_label in zip(true_sequences, predicted_sequences):\n",
    "        for true_tag, predicted_tag in zip(true_label, predicted_label):\n",
    "            total_count += 1\n",
    "            if true_tag == predicted_tag:\n",
    "                correct_count += 1\n",
    "\n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.41 s (started: 2023-09-26 22:50:55 -07:00)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the probability matrices and other data\n",
    "greedy_decoder = GreedyDecoding(p, t, e, model.states, model.vocab)\n",
    "\n",
    "# Apply Greedy Decoding on development data\n",
    "predicted_dev_tags = greedy_decoder.decode(valid_sentences_with_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Decoding Accuracy:  0.9155\n",
      "time: 47 ms (started: 2023-09-26 22:50:57 -07:00)\n"
     ]
    }
   ],
   "source": [
    "acc = calculate_accuracy(predicted_dev_tags, df_valid.labels.tolist())\n",
    "print(\"Greedy Decoding Accuracy: \", round(acc, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[influential, members, of, the, house, ways, a...</td>\n",
       "      <td>[JJ, NNS, IN, DT, NNP, NNS, CC, VBZ, NNP, VBD,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, bill, ,, whose, backers, include, chairm...</td>\n",
       "      <td>[DT, NN, ,, WP$, NNS, VBP, NN, NNP, NNP, -LRB-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[the, bill, intends, to, restrict, the, rtc, t...</td>\n",
       "      <td>[DT, NN, VBZ, TO, VB, DT, NNP, TO, NNP, NNS, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[``, such, agency, `, self-help, ', borrowing,...</td>\n",
       "      <td>[``, JJ, NN, ``, JJ, '', NN, VBZ, JJ, CC, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[the, complex, financing, plan, in, the, s&amp;l, ...</td>\n",
       "      <td>[DT, JJ, NN, NN, IN, DT, NN, NN, NN, VBZ, VBG,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  \\\n",
       "0      0  [influential, members, of, the, house, ways, a...   \n",
       "1      1  [the, bill, ,, whose, backers, include, chairm...   \n",
       "2      2  [the, bill, intends, to, restrict, the, rtc, t...   \n",
       "3      3  [``, such, agency, `, self-help, ', borrowing,...   \n",
       "4      4  [the, complex, financing, plan, in, the, s&l, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [JJ, NNS, IN, DT, NNP, NNS, CC, VBZ, NNP, VBD,...  \n",
       "1  [DT, NN, ,, WP$, NNS, VBP, NN, NNP, NNP, -LRB-...  \n",
       "2  [DT, NN, VBZ, TO, VB, DT, NNP, TO, NNP, NNS, R...  \n",
       "3  [``, JJ, NN, ``, JJ, '', NN, VBZ, JJ, CC, JJ, ...  \n",
       "4  [DT, JJ, NN, NN, IN, DT, NN, NN, NN, VBZ, VBG,...  "
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 859 ms (started: 2023-09-26 22:59:10 -07:00)\n"
     ]
    }
   ],
   "source": [
    "# Apply Greedy Decoding on Test data\n",
    "predicted_test_tags = greedy_decoder.decode(test_sentences_with_pos_tags)\n",
    "\n",
    "df_greedy_preds = df_test.copy(deep=True)\n",
    "df_greedy_preds[\"labels\"] = predicted_test_tags\n",
    "\n",
    "df_greedy_preds.to_json(PathConfig.GREEDY_ALGO_OUTPUT_PATH, orient=\"records\", indent=4)\n",
    "\n",
    "df_greedy_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-09-27 00:01:18 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class ViterbiDecoding:\n",
    "    def __init__(self, prior_probs, transition_probs, emission_probs, states, vocab):\n",
    "        self.priors = prior_probs\n",
    "        self.transitions = transition_probs\n",
    "        self.emissions = emission_probs\n",
    "        self.states = states\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.tag_to_idx = {tag: idx for idx, tag in enumerate(states)}\n",
    "        self.word_to_index = dict(zip(self.vocab[\"word\"], self.vocab[\"index\"]))\n",
    "\n",
    "        # Precompute scores for each word-tag pair\n",
    "        self.priors_emissions = prior_probs[:, np.newaxis] * emission_probs\n",
    "\n",
    "    def _decode_single_sentence(self, sentence):\n",
    "        V = np.zeros((len(sentence), len(self.states)))\n",
    "        path = np.zeros((len(sentence), len(self.states)), dtype=int)\n",
    "\n",
    "        word_idx = np.array(\n",
    "            [\n",
    "                self.word_to_index.get(word, self.word_to_index[VocabConfig.UNKNOWN_TOKEN])\n",
    "                for word in sentence\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        V[0] = np.log(self.priors_emissions[:, word_idx[0]] + 1e-10)\n",
    "\n",
    "        for t in range(1, len(sentence)):\n",
    "            scores = (\n",
    "                V[t - 1, :, np.newaxis]\n",
    "                + np.log(self.transitions + 1e-10)\n",
    "                + np.log(self.emissions[:, word_idx[t]] + 1e-10)\n",
    "            )\n",
    "            V[t] = np.max(scores, axis=0)\n",
    "            path[t] = np.argmax(scores, axis=0)\n",
    "\n",
    "        predicted_tags = [0] * len(sentence)\n",
    "        predicted_tags[-1] = np.argmax(V[-1])\n",
    "\n",
    "        for t in range(len(sentence) - 2, -1, -1):\n",
    "            predicted_tags[t] = path[t + 1, predicted_tags[t + 1]]\n",
    "\n",
    "        return [self.states[tag_idx] for tag_idx in predicted_tags]\n",
    "\n",
    "    def decode(self, sentences):\n",
    "        predicted_tags_list = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            predicted_tags = self._decode_single_sentence([word for word, tag in sentence])\n",
    "            predicted_tags_list.append(predicted_tags)\n",
    "\n",
    "        return predicted_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.03 s (started: 2023-09-27 00:01:19 -07:00)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the probability matrices and other data\n",
    "viterbi_decoder = ViterbiDecoding(p, t, e, model.states, model.vocab)\n",
    "\n",
    "# Apply Greedy Decoding on development data\n",
    "predicted_dev_tags_viterbi = viterbi_decoder.decode(valid_sentences_with_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Decoding Accuracy:  0.9323\n",
      "time: 32 ms (started: 2023-09-27 00:01:33 -07:00)\n"
     ]
    }
   ],
   "source": [
    "acc_v = calculate_accuracy(predicted_dev_tags_viterbi, df_valid.labels.tolist())\n",
    "print(\"Greedy Decoding Accuracy: \", round(acc_v, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[influential, members, of, the, house, ways, a...</td>\n",
       "      <td>[JJ, NNS, IN, DT, NNP, NNS, CC, VBZ, NNP, VBD,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, bill, ,, whose, backers, include, chairm...</td>\n",
       "      <td>[DT, NN, ,, WP$, NNS, VBP, NN, NNP, NNP, -LRB-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[the, bill, intends, to, restrict, the, rtc, t...</td>\n",
       "      <td>[DT, NN, VBZ, TO, VB, DT, NNP, TO, NNP, NNS, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[``, such, agency, `, self-help, ', borrowing,...</td>\n",
       "      <td>[``, JJ, NN, ``, JJ, '', NN, VBZ, JJ, CC, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[the, complex, financing, plan, in, the, s&amp;l, ...</td>\n",
       "      <td>[DT, JJ, NN, NN, IN, DT, NN, NN, NN, VBZ, VBG,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  \\\n",
       "0      0  [influential, members, of, the, house, ways, a...   \n",
       "1      1  [the, bill, ,, whose, backers, include, chairm...   \n",
       "2      2  [the, bill, intends, to, restrict, the, rtc, t...   \n",
       "3      3  [``, such, agency, `, self-help, ', borrowing,...   \n",
       "4      4  [the, complex, financing, plan, in, the, s&l, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [JJ, NNS, IN, DT, NNP, NNS, CC, VBZ, NNP, VBD,...  \n",
       "1  [DT, NN, ,, WP$, NNS, VBP, NN, NNP, NNP, -LRB-...  \n",
       "2  [DT, NN, VBZ, TO, VB, DT, NNP, TO, NNP, NNS, R...  \n",
       "3  [``, JJ, NN, ``, JJ, '', NN, VBZ, JJ, CC, JJ, ...  \n",
       "4  [DT, JJ, NN, NN, IN, DT, NN, NN, NN, VBZ, VBG,...  "
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 813 ms (started: 2023-09-27 00:01:37 -07:00)\n"
     ]
    }
   ],
   "source": [
    "# Apply Greedy Decoding on Test data\n",
    "predicted_test_tags_v = greedy_decoder.decode(test_sentences_with_pos_tags)\n",
    "\n",
    "df_viterbi_preds = df_test.copy(deep=True)\n",
    "df_viterbi_preds[\"labels\"] = predicted_test_tags_v\n",
    "\n",
    "df_viterbi_preds.to_json(PathConfig.VITERBI_ALGO_OUTPUT_PATH, orient=\"records\", indent=4)\n",
    "\n",
    "df_viterbi_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI544",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
