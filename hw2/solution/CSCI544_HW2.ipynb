{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-autotime in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (0.3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: ipython in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython-autotime) (8.15.0)\n",
      "Requirement already satisfied: backcall in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (5.10.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (1.1.3)\n",
      "Requirement already satisfied: colorama in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-autotime) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython->ipython-autotime) (1.16.0)\n",
      "Requirement already satisfied: contractions in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "time: 3.38 s (started: 2023-09-24 17:59:45 -07:00)\n"
     ]
    }
   ],
   "source": [
    "%pip install ipython-autotime\n",
    "%pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 0 ns (started: 2023-09-26 05:12:58 -07:00)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 05:13:00 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class PathConfig:\n",
    "    HW2_DIR = os.path.dirname(os.getcwd())\n",
    "    OUTPUT_DIR = os.path.join(HW2_DIR, \"solution\", \"output\")\n",
    "\n",
    "    DATA_PATH = os.path.join(HW2_DIR, \"CSCI544_HW2\", \"data\")\n",
    "    VERIFICATION_DATA_PATH = os.path.join(HW2_DIR, \"CSCI544_HW2\", \"verification\")\n",
    "\n",
    "    VOCAB_FILE_PATH = os.path.join(OUTPUT_DIR, \"vocab.txt\")\n",
    "    HMM_MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, \"hmm.json\")\n",
    "\n",
    "\n",
    "class WSJDatasetConfig:\n",
    "    cols = [\"index\", \"sentences\", \"labels\"]\n",
    "    \n",
    "    train_file_path = os.path.join(PathConfig.DATA_PATH, \"train.json\")\n",
    "    dev_file_path = os.path.join(PathConfig.DATA_PATH, \"dev.json\")\n",
    "    test_file_path = os.path.join(PathConfig.DATA_PATH, \"test.json\")\n",
    "\n",
    "\n",
    "class VocabConfig:\n",
    "    UNKNOWN_TOKEN = \"<unk>\"\n",
    "    THRESHOLD = 3\n",
    "    FILE_HEADER = [\"word\", \"index\", \"frequency\"]\n",
    "\n",
    "    VOCAB_FILE = PathConfig.VOCAB_FILE_PATH\n",
    "\n",
    "\n",
    "class HMMConfig:\n",
    "    HMM_MODEL_SAVED = PathConfig.HMM_MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2023-09-26 05:13:01 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class WSJDataset:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "        self.data: pd.DataFrame = None\n",
    "        self.cols = WSJDatasetConfig.cols\n",
    "\n",
    "    def read_data(self):\n",
    "        self.data = pd.read_json(self.path)\n",
    "\n",
    "    def process_sentences(self):\n",
    "        self.data[\"sentence\"] = self.data[\"sentence\"].apply(\n",
    "            lambda sentence: ' '.join(sentence).lower().split()\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        self.read_data()\n",
    "        self.process_sentences()\n",
    "        return self.data\n",
    "    \n",
    "    def get_sentences_with_pos_tags(self):\n",
    "        sentences_with_pos_tags = self.data.loc[:, [\"sentence\", \"labels\"]].apply(\n",
    "            lambda row: list(zip(row[\"sentence\"], row[\"labels\"])), axis=1\n",
    "        )\n",
    "        sentences_with_pos_tags = sentences_with_pos_tags.tolist()\n",
    "        return sentences_with_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[mr., vinken, is, chairman, of, elsevier, n.v....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[rudolph, agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[a, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[the, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  \\\n",
       "0      0  [pierre, vinken, ,, 61, years, old, ,, will, j...   \n",
       "1      1  [mr., vinken, is, chairman, of, elsevier, n.v....   \n",
       "2      2  [rudolph, agnew, ,, 55, years, old, and, forme...   \n",
       "3      3  [a, form, of, asbestos, once, used, to, make, ...   \n",
       "4      4  [the, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  \n",
       "1  [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  \n",
       "2  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  \n",
       "3  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "4  [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.28 s (started: 2023-09-26 05:13:02 -07:00)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = WSJDataset(path=WSJDatasetConfig.train_file_path)\n",
    "df_train = train_dataset.prepare_dataset()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[the, arizona, corporations, commission, autho...</td>\n",
       "      <td>[DT, NNP, NNP, NNP, VBD, DT, CD, NN, NN, NN, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, ruling, follows, a, host, of, problems, ...</td>\n",
       "      <td>[DT, NN, VBZ, DT, NN, IN, NNS, IN, NNP, NNP, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[the, arizona, regulatory, ruling, calls, for,...</td>\n",
       "      <td>[DT, NNP, JJ, NN, VBZ, IN, $, CD, CD, IN, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[the, company, had, sought, increases, totalin...</td>\n",
       "      <td>[DT, NN, VBD, VBN, NNS, VBG, $, CD, CD, ,, CC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[the, decision, was, announced, after, trading...</td>\n",
       "      <td>[DT, NN, VBD, VBN, IN, NN, VBD, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  \\\n",
       "0      0  [the, arizona, corporations, commission, autho...   \n",
       "1      1  [the, ruling, follows, a, host, of, problems, ...   \n",
       "2      2  [the, arizona, regulatory, ruling, calls, for,...   \n",
       "3      3  [the, company, had, sought, increases, totalin...   \n",
       "4      4  [the, decision, was, announced, after, trading...   \n",
       "\n",
       "                                              labels  \n",
       "0  [DT, NNP, NNP, NNP, VBD, DT, CD, NN, NN, NN, I...  \n",
       "1  [DT, NN, VBZ, DT, NN, IN, NNS, IN, NNP, NNP, ,...  \n",
       "2  [DT, NNP, JJ, NN, VBZ, IN, $, CD, CD, IN, JJ, ...  \n",
       "3  [DT, NN, VBD, VBN, NNS, VBG, $, CD, CD, ,, CC,...  \n",
       "4                 [DT, NN, VBD, VBN, IN, NN, VBD, .]  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 234 ms (started: 2023-09-26 05:13:03 -07:00)\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = WSJDataset(path=WSJDatasetConfig.dev_file_path)\n",
    "df_valid = valid_dataset.prepare_dataset()\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[influential, members, of, the, house, ways, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[the, bill, ,, whose, backers, include, chairm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[the, bill, intends, to, restrict, the, rtc, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[``, such, agency, `, self-help, ', borrowing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[the, complex, financing, plan, in, the, s&amp;l, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence\n",
       "0      0  [influential, members, of, the, house, ways, a...\n",
       "1      1  [the, bill, ,, whose, backers, include, chairm...\n",
       "2      2  [the, bill, intends, to, restrict, the, rtc, t...\n",
       "3      3  [``, such, agency, `, self-help, ', borrowing,...\n",
       "4      4  [the, complex, financing, plan, in, the, s&l, ..."
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 188 ms (started: 2023-09-26 05:13:04 -07:00)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = WSJDataset(path=WSJDatasetConfig.test_file_path)\n",
    "df_test = test_dataset.prepare_dataset()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Vocabulary Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2023-09-26 05:13:06 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class VocabularyGenerator:\n",
    "    def __init__(\n",
    "        self, threshold: int, unknown_token: str = None, save: bool = False, path: str = None\n",
    "    ):\n",
    "        \"\"\"Initialize a VocabularyGenerator\n",
    "\n",
    "        Args:\n",
    "            threshold (int): Frequency threshold for rare words.\n",
    "            unknown_token (str, optional): Token to replace rare words. Defaults to None.\n",
    "            save (bool, optional): Flag to save the vocabulary. Default is True.\n",
    "            path (str, optional): Path to save the vocabulary. Defaults to None.\n",
    "\n",
    "        Usage:\n",
    "            vocab_generator = VocabularyGenerator(threshold=3, unknown_token=\"<unk>\")\n",
    "            vocab_df = vocab_generator.generate_vocabulary(data, \"sentence\")\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.unknown_token = (\n",
    "            unknown_token if unknown_token is not None else VocabConfig.UNKNOWN_TOKEN\n",
    "        )\n",
    "        self._save = save\n",
    "\n",
    "        if self._save and path is None:\n",
    "            self.path = VocabConfig.VOCAB_FILE\n",
    "        else:\n",
    "            self.path = path\n",
    "\n",
    "    def _count_word_frequency(self, data, sentence_col_name):\n",
    "        word_freq = (\n",
    "            data[sentence_col_name]\n",
    "            .explode()\n",
    "            .value_counts()\n",
    "            .rename_axis(\"word\")\n",
    "            .reset_index(name=\"frequency\")\n",
    "        )\n",
    "        return word_freq\n",
    "\n",
    "    def generate_vocabulary(self, data: pd.DataFrame, sentence_col_name: str):\n",
    "        \"\"\"Generate a vocabulary from the provided dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The DataFrame containing the dataset.\n",
    "            sentence_col_name (str): The name of the column containing sentences.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with the generated vocabulary.\n",
    "\n",
    "        This method takes a DataFrame with sentences and generates a vocabulary based on word frequencies.\n",
    "        It replaces words with frequencies less than the specified threshold with the unknown token (\"<unk>\").\n",
    "        The resulting DataFrame is sorted by frequency and indexed.\n",
    "\n",
    "        If the 'save' flag is set, the vocabulary will be saved to the specified path.\n",
    "\n",
    "        Usage:\n",
    "            ```py\n",
    "            vocab_generator = VocabularyGenerator(threshold=3, unknown_token=\"<unk>\")\n",
    "            vocab_df = vocab_generator.generate_vocabulary(data, sentence_col_name)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        word_freq_df = self._count_word_frequency(data, sentence_col_name)\n",
    "\n",
    "        # Replace words with frequency less than threshold with '<unk>'\n",
    "        word_freq_df[\"word\"] = word_freq_df.apply(\n",
    "            lambda row: self.unknown_token if row[\"frequency\"] <= self.threshold else row[\"word\"],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # # Group by 'Word' and aggregate by sum\n",
    "        word_freq_df = word_freq_df.groupby(\"word\", as_index=False)[\"frequency\"].agg(\"sum\")\n",
    "\n",
    "        # Sort the DataFrame by frequency\n",
    "        word_freq_df = word_freq_df.sort_values(by=\"frequency\", ascending=False, ignore_index=True)\n",
    "\n",
    "        # Add an index column\n",
    "        word_freq_df[\"index\"] = range(1, len(word_freq_df) + 1)\n",
    "\n",
    "        if self._save:\n",
    "            self.save_vocab(word_freq_df, self.path)\n",
    "\n",
    "        return word_freq_df\n",
    "\n",
    "    def save_vocab(self, word_freq_df, path):\n",
    "        \"\"\"Write your vocabulary to the file\"\"\"\n",
    "        if not os.path.exists(os.path.dirname(path)):\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "\n",
    "        with open(path, \"w\") as file:\n",
    "            vocabulary = word_freq_df.to_records(index=False)\n",
    "            for word, frequency, index in vocabulary:\n",
    "                file.write(f\"{word}\\t{index}\\t{frequency}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>46476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>46144</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>37452</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>37245</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>22176</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>21459</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>19338</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>16320</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and</td>\n",
       "      <td>15875</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'s</td>\n",
       "      <td>8886</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  frequency  index\n",
       "0      ,      46476      1\n",
       "1    the      46144      2\n",
       "2      .      37452      3\n",
       "3  <unk>      37245      4\n",
       "4     of      22176      5\n",
       "5     to      21459      6\n",
       "6      a      19338      7\n",
       "7     in      16320      8\n",
       "8    and      15875      9\n",
       "9     's       8886     10"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 984 ms (started: 2023-09-26 05:13:09 -07:00)\n"
     ]
    }
   ],
   "source": [
    "vocab_generator = VocabularyGenerator(\n",
    "    threshold=3, unknown_token=VocabConfig.UNKNOWN_TOKEN, save=True\n",
    ")\n",
    "vocab_df = vocab_generator.generate_vocabulary(df_train, \"sentence\")\n",
    "vocab_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected threshold for unknown words:  3\n",
      "Total size of the vocabulary:  12680\n",
      "Total occurrences of the special token <unk>:  37245\n",
      "time: 16 ms (started: 2023-09-26 05:13:12 -07:00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected threshold for unknown words: \", VocabConfig.THRESHOLD)\n",
    "print(\"Total size of the vocabulary: \", vocab_df.shape[0])\n",
    "print(\n",
    "    \"Total occurrences of the special token <unk>: \",\n",
    "    int(vocab_df[vocab_df[\"word\"] == \"<unk>\"].frequency),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique POS tags = 45\n",
      "time: 250 ms (started: 2023-09-26 05:13:16 -07:00)\n"
     ]
    }
   ],
   "source": [
    "df_pos = df_train.labels.explode().value_counts().reset_index(name=\"count\")\n",
    "print(\"Number of unique POS tags =\", df_pos.shape[0])\n",
    "df_pos = df_pos.labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.38 s (started: 2023-09-26 05:13:17 -07:00)\n"
     ]
    }
   ],
   "source": [
    "train_sentences_with_pos_tags = train_dataset.get_sentences_with_pos_tags()\n",
    "valid_sentences_with_pos_tags = valid_dataset.get_sentences_with_pos_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pierre', 'NNP'),\n",
       " ('vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 05:13:19 -07:00)\n"
     ]
    }
   ],
   "source": [
    "train_sentences_with_pos_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2023-09-26 05:13:31 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class HMM:\n",
    "    def __init__(self, vocab_file: str, labels: List[str]):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            train_data (pd.DataFrame): _description_\n",
    "            vocab_file (str): _description_\n",
    "        \"\"\"\n",
    "        self.vocab = self._read_vocab(vocab_file)\n",
    "        self.labels = labels\n",
    "\n",
    "        # Hidden Markov Model Parameters\n",
    "        self.states = list()\n",
    "        self.priors = None\n",
    "        self.transitions = None\n",
    "        self.emissions = None\n",
    "\n",
    "    def _read_vocab(self, vocab_file: str):\n",
    "        return pd.read_csv(vocab_file, sep=\"\\t\", names=VocabConfig.FILE_HEADER)\n",
    "\n",
    "    def _initialize_params(self):\n",
    "        self.states = list(self.labels)\n",
    "\n",
    "        # N = Number of states i.e. number of distinct tags\n",
    "        num_states = len(self.labels)\n",
    "        # M = Number of observable symbols i.e. number of distinct words\n",
    "        num_observations = len(self.vocab)\n",
    "\n",
    "        # State transition probability matrix of size N * N\n",
    "        self.transitions = np.zeros((num_states, num_states))\n",
    "\n",
    "        # Obseravtion Emission probability matrix of size N * M\n",
    "        self.emissions = np.zeros((num_states, num_observations))\n",
    "\n",
    "        # Prior probability matrix of size N * 1\n",
    "        self.prior = np.ones(num_states)\n",
    "\n",
    "    def _compute_prior_params(self, train_data):\n",
    "        num_sentences = len(train_data)\n",
    "\n",
    "        state_occurrence = Counter()\n",
    "\n",
    "        for sentence in train_data:\n",
    "            # Ensure the sentence is not empty\n",
    "            if sentence:\n",
    "                # Get the label of the first word in the sentence\n",
    "                label = sentence[0][1]\n",
    "                state_occurrence[label] += 1\n",
    "\n",
    "        self.priors = np.array([state_occurrence[state] / num_sentences for state in self.labels])\n",
    "\n",
    "    def _compute_transition_params(self, train_data):\n",
    "        labels_list = [label for sentence in train_data for _, label in sentence]\n",
    "        label_indices = [self.states.index(label) for label in labels_list]\n",
    "\n",
    "        for i in range(len(label_indices) - 1):\n",
    "            curr_state = label_indices[i]\n",
    "            next_state = label_indices[i + 1]\n",
    "            self.transitions[curr_state, next_state] += 1\n",
    "\n",
    "        # Handle cases where the probabilities is 0\n",
    "        self.transitions = np.where(self.transitions == 0, 1e-10, self.transitions)\n",
    "\n",
    "        row_agg = self.transitions.sum(axis=1)\n",
    "        self.transitions = self.transitions / row_agg[:, np.newaxis]\n",
    "\n",
    "    def _compute_emission_params(self, train_data):\n",
    "        word_to_index = dict(zip(self.vocab[\"word\"], self.vocab[\"index\"]))\n",
    "\n",
    "        for sentence in train_data:\n",
    "            for word, label in sentence:\n",
    "                state_idx = self.states.index(label)\n",
    "                word_idx = word_to_index.get(word, word_to_index[VocabConfig.UNKNOWN_TOKEN]) - 1\n",
    "                self.emissions[state_idx, word_idx] += 1\n",
    "\n",
    "        # Handle cases where the probabilities is 0\n",
    "        self.emissions = np.where(self.emissions == 0, 1e-10, self.emissions)\n",
    "\n",
    "        row_agg = self.emissions.sum(axis=1)\n",
    "        self.emissions = self.emissions / row_agg[:, np.newaxis]\n",
    "\n",
    "    def fit(self, train_data: pd.DataFrame):\n",
    "        self._initialize_params()\n",
    "        self._compute_prior_params(train_data)\n",
    "        self._compute_transition_params(train_data)\n",
    "        self._compute_emission_params(train_data)\n",
    "\n",
    "    @property\n",
    "    def get_all_probability_matrices(self):\n",
    "        return self.priors, self.transitions, self.emissions\n",
    "\n",
    "    def save_model(self, file_path=None):\n",
    "        if file_path is None:\n",
    "            file_path = HMMConfig.HMM_MODEL_SAVED\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "\n",
    "        transition_prob = {\n",
    "            f\"({s1}, {s2})\": self.transitions[self.states.index(s1), self.states.index(s2)]\n",
    "            for s1, s2 in itertools.product(self.states, repeat=2)\n",
    "        }\n",
    "\n",
    "        emission_prob = {\n",
    "            f\"({s}, {w})\": p\n",
    "            for s in self.states\n",
    "            for w, p in zip(self.vocab[\"word\"], self.emissions[self.states.index(s), :])\n",
    "        }\n",
    "\n",
    "        model_params = {\"transition\": transition_prob, \"emission\": emission_prob}\n",
    "\n",
    "        with open(file_path, \"w\") as json_file:\n",
    "            json.dump(model_params, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.5 s (started: 2023-09-26 05:13:36 -07:00)\n"
     ]
    }
   ],
   "source": [
    "model = HMM(vocab_file=VocabConfig.VOCAB_FILE, labels=df_pos)\n",
    "model.fit(train_sentences_with_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Transition Parameters = 2025\n",
      "Number of Emission Parameters = 570600\n",
      "time: 15 ms (started: 2023-09-26 05:13:42 -07:00)\n"
     ]
    }
   ],
   "source": [
    "p, t, e = model.get_all_probability_matrices\n",
    "print(\"Number of Transition Parameters =\", len(t.flatten()))\n",
    "print(\"Number of Emission Parameters =\", len(e.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.83 s (started: 2023-09-26 05:13:45 -07:00)\n"
     ]
    }
   ],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-09-26 05:53:23 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class GreedyDecoding:\n",
    "    def __init__(self, prior_probs, transition_probs, emission_probs, states, vocab):\n",
    "        self.priors = prior_probs\n",
    "        self.transitions = transition_probs\n",
    "        self.emissions = emission_probs\n",
    "        self.states = states\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.tag_to_idx = {tag: idx for idx, tag in enumerate(states)}\n",
    "        self.word_to_index = dict(zip(self.vocab[\"word\"], self.vocab[\"index\"]))\n",
    "\n",
    "        # Precompute scores for each word-tag pair\n",
    "        num_words = len(vocab)\n",
    "        num_tags = len(states)\n",
    "        self.scores = np.zeros((num_words, num_tags))\n",
    "\n",
    "        for i, word in enumerate(vocab[\"word\"]):\n",
    "            for j, tag in enumerate(states):\n",
    "                self.scores[i, j] = prior_probs[j] * emission_probs[j, i]\n",
    "\n",
    "    def _decode_single_sentence(self, sentence):\n",
    "        predicted_tags = []\n",
    "\n",
    "        # Process the first word\n",
    "        word_idx = (\n",
    "            self.word_to_index.get(sentence[0], self.word_to_index[VocabConfig.UNKNOWN_TOKEN]) - 1\n",
    "        )\n",
    "        predicted_tag_idx = np.argmax(self.scores[word_idx])\n",
    "        predicted_tags.append(self.states[predicted_tag_idx])\n",
    "\n",
    "        # Process the rest of the sentence\n",
    "        for i in range(1, len(sentence)):\n",
    "            word = sentence[i]\n",
    "            word_idx = (\n",
    "                self.word_to_index.get(word, self.word_to_index[VocabConfig.UNKNOWN_TOKEN]) - 1\n",
    "            )\n",
    "\n",
    "            # Calculate scores for all tags in one step\n",
    "            scores = self.transitions[:, predicted_tag_idx] * self.emissions[:, word_idx]\n",
    "\n",
    "            # Find the tag with the highest score\n",
    "            predicted_tag_idx = np.argmax(scores)\n",
    "            predicted_tags.append(self.states[predicted_tag_idx])\n",
    "\n",
    "        return predicted_tags\n",
    "\n",
    "    def decode(self, sentences):\n",
    "        predicted_tags_list = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            predicted_tags = self._decode_single_sentence([word for word, tag in sentence])\n",
    "            predicted_tags_list.append(predicted_tags)\n",
    "\n",
    "        return predicted_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-26 05:54:41 -07:00)\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predicted_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predicted labels compared to true labels.\n",
    "\n",
    "    Args:\n",
    "        predicted_labels (list): List of predicted labels.\n",
    "        true_labels (list): List of true labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy as a percentage (0.0 to 100.0).\n",
    "    \"\"\"\n",
    "    assert len(predicted_labels) == len(true_labels), \"Lists must have the same length.\"\n",
    "\n",
    "    correct_predictions = sum(1 for pred, true in zip(predicted_labels, true_labels) if pred == true)\n",
    "    total_predictions = len(predicted_labels)\n",
    "\n",
    "    accuracy = (correct_predictions / total_predictions) * 100.0\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.14 s (started: 2023-09-26 05:53:25 -07:00)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the probability matrices and other data\n",
    "greedy_decoder = GreedyDecoding(p, t, e, model.states, model.vocab)\n",
    "\n",
    "# Apply Greedy Decoding on development data\n",
    "predicted_dev_tags = greedy_decoder.decode(valid_sentences_with_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.016645558168989"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-09-26 05:55:18 -07:00)\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(predicted_dev_tags, df_valid.labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI544",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
