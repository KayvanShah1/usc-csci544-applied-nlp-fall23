{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-autotime in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (0.3.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: ipython in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython-autotime) (8.15.0)\n",
      "Requirement already satisfied: backcall in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (2.16.1)\n",
      "Requirement already satisfied: stack-data in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (5.10.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (1.1.3)\n",
      "Requirement already satisfied: colorama in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from ipython->ipython-autotime) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->ipython-autotime) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from stack-data->ipython->ipython-autotime) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython->ipython-autotime) (1.16.0)\n",
      "Requirement already satisfied: contractions in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\binaries\\python_envs\\csci544\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "time: 3.38 s (started: 2023-09-24 17:59:45 -07:00)\n"
     ]
    }
   ],
   "source": [
    "%pip install ipython-autotime\n",
    "%pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 0 ns (started: 2023-09-25 01:44:10 -07:00)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-24 22:00:58 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class PathConfig:\n",
    "    HW2_DIR = os.path.dirname(os.getcwd())\n",
    "    OUTPUT_DIR = os.path.join(HW2_DIR, \"solution\", \"output\")\n",
    "\n",
    "    DATA_PATH = os.path.join(HW2_DIR, \"CSCI544_HW2\", \"data\")\n",
    "    VERIFICATION_DATA_PATH = os.path.join(HW2_DIR, \"CSCI544_HW2\", \"verification\")\n",
    "\n",
    "    VOCAB_FILE_PATH = os.path.join(OUTPUT_DIR, \"vocab.txt\")\n",
    "    HMM_MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, \"hmm.json\")\n",
    "\n",
    "\n",
    "class VocabConfig:\n",
    "    UNKNOWN_TOKEN = \"<unk>\"\n",
    "    THRESHOLD = 3\n",
    "    FILE_HEADER = [\"word\", \"index\", \"frequency\"]\n",
    "\n",
    "    VOCAB_FILE = PathConfig.VOCAB_FILE_PATH\n",
    "\n",
    "\n",
    "class HMMConfig:\n",
    "    HMM_MODEL_SAVED = PathConfig.HMM_MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Vocabulary Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-25 01:09:21 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class VocabularyGenerator:\n",
    "    def __init__(\n",
    "        self, threshold: int, unknown_token: str = None, save: bool = False, path: str = None\n",
    "    ):\n",
    "        \"\"\"Initialize a VocabularyGenerator\n",
    "\n",
    "        Args:\n",
    "            threshold (int): Frequency threshold for rare words.\n",
    "            unknown_token (str, optional): Token to replace rare words. Defaults to None.\n",
    "            save (bool, optional): Flag to save the vocabulary. Default is True.\n",
    "            path (str, optional): Path to save the vocabulary. Defaults to None.\n",
    "\n",
    "        Usage:\n",
    "            vocab_generator = VocabularyGenerator(threshold=3, unknown_token=\"<unk>\")\n",
    "            vocab_df = vocab_generator.generate_vocabulary(data, \"sentence\")\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.unknown_token = (\n",
    "            unknown_token if unknown_token is not None else VocabConfig.UNKNOWN_TOKEN\n",
    "        )\n",
    "        self._save = save\n",
    "\n",
    "        if self._save and path is None:\n",
    "            self.path = VocabConfig.VOCAB_FILE\n",
    "        else:\n",
    "            self.path = path\n",
    "\n",
    "    def _count_word_frequency(self, data, sentence_col_name):\n",
    "        word_freq = (\n",
    "            data[sentence_col_name]\n",
    "            .explode()\n",
    "            .value_counts()\n",
    "            .rename_axis(\"word\")\n",
    "            .reset_index(name=\"frequency\")\n",
    "        )\n",
    "        return word_freq\n",
    "\n",
    "    def generate_vocabulary(self, data: pd.DataFrame, sentence_col_name: str):\n",
    "        \"\"\"Generate a vocabulary from the provided dataset.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The DataFrame containing the dataset.\n",
    "            sentence_col_name (str): The name of the column containing sentences.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with the generated vocabulary.\n",
    "\n",
    "        This method takes a DataFrame with sentences and generates a vocabulary based on word frequencies.\n",
    "        It replaces words with frequencies less than the specified threshold with the unknown token (\"<unk>\").\n",
    "        The resulting DataFrame is sorted by frequency and indexed.\n",
    "\n",
    "        If the 'save' flag is set, the vocabulary will be saved to the specified path.\n",
    "\n",
    "        Usage:\n",
    "            ```py\n",
    "            vocab_generator = VocabularyGenerator(threshold=3, unknown_token=\"<unk>\")\n",
    "            vocab_df = vocab_generator.generate_vocabulary(data, sentence_col_name)\n",
    "            ```\n",
    "        \"\"\"\n",
    "        word_freq_df = self._count_word_frequency(data, sentence_col_name)\n",
    "\n",
    "        # Create a DataFrame\n",
    "        # word_freq_df = pd.DataFrame(word_freq_list, columns=[\"word\", \"frequency\"])\n",
    "\n",
    "        # Replace words with frequency less than threshold with '<unk>'\n",
    "        word_freq_df[\"word\"] = word_freq_df.apply(\n",
    "            lambda row: self.unknown_token if row[\"frequency\"] <= self.threshold else row[\"word\"],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # # Group by 'Word' and aggregate by sum\n",
    "        word_freq_df = word_freq_df.groupby(\"word\", as_index=False)[\"frequency\"].agg(\"sum\")\n",
    "\n",
    "        # Sort the DataFrame by frequency\n",
    "        word_freq_df = word_freq_df.sort_values(by=\"frequency\", ascending=False, ignore_index=True)\n",
    "\n",
    "        # Add an index column\n",
    "        word_freq_df[\"index\"] = range(1, len(word_freq_df) + 1)\n",
    "\n",
    "        if self._save:\n",
    "            self.save_vocab(word_freq_df, self.path)\n",
    "\n",
    "        return word_freq_df\n",
    "\n",
    "    def save_vocab(self, word_freq_df, path):\n",
    "        \"\"\"Write your vocabulary to the file\"\"\"\n",
    "        if not os.path.exists(os.path.dirname(path)):\n",
    "            os.makedirs(os.path.dirname(path))\n",
    "\n",
    "        with open(path, \"w\") as file:\n",
    "            vocabulary = word_freq_df.to_records(index=False)\n",
    "            for word, frequency, index in vocabulary:\n",
    "                file.write(f\"{word}\\t{index}\\t{frequency}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>,</td>\n",
       "      <td>46476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>42044</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>39533</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>37452</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>22104</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>21305</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>18469</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and</td>\n",
       "      <td>15346</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>14609</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'s</td>\n",
       "      <td>8872</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  frequency  index\n",
       "0      ,      46476      1\n",
       "1  <unk>      42044      2\n",
       "2    the      39533      3\n",
       "3      .      37452      4\n",
       "4     of      22104      5\n",
       "5     to      21305      6\n",
       "6      a      18469      7\n",
       "7    and      15346      8\n",
       "8     in      14609      9\n",
       "9     's       8872     10"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.78 s (started: 2023-09-25 01:09:23 -07:00)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(PathConfig.DATA_PATH, \"train.json\"))\n",
    "\n",
    "vocab_generator = VocabularyGenerator(threshold=3, unknown_token=\"<unk>\", save=True)\n",
    "vocab_df = vocab_generator.generate_vocabulary(df, \"sentence\")\n",
    "vocab_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected threshold for unknown words:  3\n",
      "Total size of the vocabulary:  13751\n",
      "Total occurrences of the special token <unk>:  42044\n",
      "time: 16 ms (started: 2023-09-24 22:45:44 -07:00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected threshold for unknown words: \", VocabConfig.THRESHOLD)\n",
    "print(\"Total size of the vocabulary: \", vocab_df.shape[0])\n",
    "print(\n",
    "    \"Total occurrences of the special token <unk>: \",\n",
    "    int(vocab_df[vocab_df[\"word\"] == \"<unk>\"].frequency),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique POS tags = 45\n",
      "time: 172 ms (started: 2023-09-24 23:30:03 -07:00)\n"
     ]
    }
   ],
   "source": [
    "df_pos = df.labels.explode().value_counts().reset_index(name=\"count\")\n",
    "print(\"Number of unique POS tags =\", df_pos.shape[0])\n",
    "df_pos = df_pos.labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('61', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('old', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('will', 'MD'),\n",
       "  ('join', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('board', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('nonexecutive', 'JJ'),\n",
       "  ('director', 'NN'),\n",
       "  ('Nov.', 'NNP'),\n",
       "  ('29', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('chairman', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Elsevier', 'NNP'),\n",
       "  ('N.V.', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('Dutch', 'NNP'),\n",
       "  ('publishing', 'VBG'),\n",
       "  ('group', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.25 s (started: 2023-09-24 23:07:11 -07:00)\n"
     ]
    }
   ],
   "source": [
    "sentences_with_pos_tags = df.loc[:, [\"sentence\", \"labels\"]].apply(\n",
    "    lambda row: list(zip(row[\"sentence\"], row[\"labels\"])), axis=1\n",
    ")\n",
    "sentences_with_pos_tags = sentences_with_pos_tags.tolist()\n",
    "sentences_with_pos_tags[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-09-25 01:44:51 -07:00)\n"
     ]
    }
   ],
   "source": [
    "class HMM:\n",
    "    def __init__(self, vocab_file: str, labels: List[str]):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            train_data (pd.DataFrame): _description_\n",
    "            vocab_file (str): _description_\n",
    "        \"\"\"\n",
    "        self.vocab = self._read_vocab(vocab_file)\n",
    "        self.labels = labels\n",
    "\n",
    "        # Hidden Markov Model Parameters\n",
    "        self.states = list()\n",
    "        self.priors = None\n",
    "        self.transitions = None\n",
    "        self.emissions = None\n",
    "\n",
    "    def _read_vocab(self, vocab_file: str):\n",
    "        return pd.read_csv(vocab_file, sep=\"\\t\", names=VocabConfig.FILE_HEADER)\n",
    "\n",
    "    def _initialize_params(self):\n",
    "        self.states = list(self.labels)\n",
    "\n",
    "        # N = Number of states i.e. number of distinct tags\n",
    "        num_states = len(self.labels)\n",
    "        # M = Number of observable symbols i.e. number of distinct words\n",
    "        num_observations = len(self.vocab)\n",
    "\n",
    "        # State transition probability matrix of size N * N\n",
    "        self.transitions = np.zeros((num_states, num_states))\n",
    "\n",
    "        # Obseravtion Emission probability matrix of size N * M\n",
    "        self.emissions = np.zeros((num_states, num_observations))\n",
    "\n",
    "        # Prior probability matrix of size N * 1\n",
    "        self.prior = np.ones(num_states)\n",
    "\n",
    "    def _compute_prior_params(self, train_data):\n",
    "        num_sentences = len(train_data)\n",
    "\n",
    "        state_occurrence = Counter()\n",
    "\n",
    "        for sentence in train_data:\n",
    "            # Ensure the sentence is not empty\n",
    "            if sentence:\n",
    "                # Get the label of the first word in the sentence\n",
    "                label = sentence[0][1]\n",
    "                state_occurrence[label] += 1\n",
    "\n",
    "        self.priors = np.array([state_occurrence[state] / num_sentences for state in self.labels])\n",
    "\n",
    "    def _compute_transition_params(self, train_data):\n",
    "        labels_list = [label for sentence in train_data for _, label in sentence]\n",
    "        label_indices = [self.states.index(label) for label in labels_list]\n",
    "\n",
    "        for i in range(len(label_indices) - 1):\n",
    "            curr_state = label_indices[i]\n",
    "            next_state = label_indices[i + 1]\n",
    "            self.transitions[curr_state, next_state] += 1\n",
    "\n",
    "        # Handle cases where the probabilities is 0\n",
    "        self.transitions = np.where(self.transitions == 0, 1e-10, self.transitions)\n",
    "\n",
    "        row_agg = self.transitions.sum(axis=1)\n",
    "        self.transitions = self.transitions / row_agg[:, np.newaxis]\n",
    "\n",
    "    def _compute_emission_params(self, train_data):\n",
    "        word_to_index = dict(zip(self.vocab[\"word\"], self.vocab[\"index\"]))\n",
    "\n",
    "        for sentence in train_data:\n",
    "            for word, label in sentence:\n",
    "                state_idx = self.states.index(label)\n",
    "                word_idx = word_to_index.get(word, word_to_index[VocabConfig.UNKNOWN_TOKEN]) - 1\n",
    "                self.emissions[state_idx, word_idx] += 1\n",
    "\n",
    "        # Handle cases where the probabilities is 0\n",
    "        self.emissions = np.where(self.emissions == 0, 1e-10, self.emissions)\n",
    "\n",
    "        row_agg = self.emissions.sum(axis=1)\n",
    "        self.emissions = self.emissions / row_agg[:, np.newaxis]\n",
    "\n",
    "    def fit(self, train_data: pd.DataFrame):\n",
    "        self._initialize_params()\n",
    "        self._compute_prior_params(train_data)\n",
    "        self._compute_transition_params(train_data)\n",
    "        self._compute_emission_params(train_data)\n",
    "\n",
    "    @property\n",
    "    def get_all_probability_matrices(self):\n",
    "        return self.priors, self.transitions, self.emissions\n",
    "\n",
    "    def save_model(self, file_path=None):\n",
    "        if file_path is None:\n",
    "            file_path = HMMConfig.HMM_MODEL_SAVED\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(file_path)):\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "\n",
    "        transition_prob = {\n",
    "            f\"({s1}, {s2})\": self.transitions[self.states.index(s1), self.states.index(s2)]\n",
    "            for s1, s2 in itertools.product(self.states, repeat=2)\n",
    "        }\n",
    "\n",
    "        emission_prob = {\n",
    "            f\"({s}, {w})\": p\n",
    "            for s in self.states\n",
    "            for w, p in zip(self.vocab[\"word\"], self.emissions[self.states.index(s), :])\n",
    "        }\n",
    "\n",
    "        model_params = {\"transition\": transition_prob, \"emission\": emission_prob}\n",
    "\n",
    "        with open(file_path, \"w\") as json_file:\n",
    "            json.dump(model_params, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.88 s (started: 2023-09-25 01:45:09 -07:00)\n"
     ]
    }
   ],
   "source": [
    "model = HMM(vocab_file=VocabConfig.VOCAB_FILE, labels=df_pos)\n",
    "model.fit(sentences_with_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Transition Parameters = 2025\n",
      "Number of Emission Parameters = 618795\n",
      "time: 0 ns (started: 2023-09-25 01:45:15 -07:00)\n"
     ]
    }
   ],
   "source": [
    "p, t, e = model.get_all_probability_matrices\n",
    "print(\"Number of Transition Parameters =\", len(t.flatten()))\n",
    "print(\"Number of Emission Parameters =\", len(e.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.03 s (started: 2023-09-25 01:45:20 -07:00)\n"
     ]
    }
   ],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyDecoding:\n",
    "    def __init__(self, prior_probs, transition_probs, emission_probs, states, vocab):\n",
    "        self.priors = prior_probs\n",
    "        self.transitions = transition_probs\n",
    "        self.emissions = emission_probs\n",
    "        self.states = states\n",
    "        self.vocab = vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding with HMM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI544",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
